---
title: "ZeroEval Python SDK"
description: "LLM Stats is powered by ZeroEval, our in-house evaluation library"
---

<Card
  title="Start with the SDK quickstart"
  icon="rocket"
  href="/python-sdk/quickstart"
  horizontal
>
  Build your first dataset, run a task, and compute quality metrics.
</Card>

<Info>
  **About the stack:** LLM Stats runs on top of **ZeroEval**, the evaluation library developed by the same team behind LLM Stats.
</Info>

## What You Can Do

<CardGroup cols={2}>
  <Card title="Create datasets" icon="database" href="/python-sdk/datasets/creating">
    Build datasets from Python lists or CSV files and push them to ZeroEval.
  </Card>
  <Card title="Load and inspect data" icon="table-columns" href="/python-sdk/datasets/loading">
    Pull datasets, access rows with dot notation, and work with slices/subsets.
  </Card>
  <Card title="Run evals" icon="play" href="/python-sdk/evals/running-evals">
    Execute tasks with configurable workers, retries, and checkpoints.
  </Card>
  <Card title="Score quality" icon="chart-line" href="/python-sdk/evals/scoring-and-metrics">
    Add row, column, and run-level evaluations for robust measurement.
  </Card>
</CardGroup>

## Recommended Path

<Steps>
  <Step title="Install and authenticate">
    Follow [`/python-sdk/installation`](/python-sdk/installation) to install `zeroeval` and configure `ZEROEVAL_API_KEY`.
  </Step>
  <Step title="Complete the first eval">
    Run the walkthrough in [`/python-sdk/quickstart`](/python-sdk/quickstart).
  </Step>
  <Step title="Master datasets">
    Learn creation, loading, versioning/subsets, and multimodal data handling.
  </Step>
  <Step title="Productionize eval execution">
    Add scoring, repetition, and resume/reliability controls.
  </Step>
</Steps>

## Documentation Scope

- **Getting Started**: setup and first run
- **Datasets**: creation, loading, versioning, subsets, multimodal
- **Evals**: execution, scoring, metrics, repetitions, resume
- **Examples**: end-to-end text and multimodal workflows
